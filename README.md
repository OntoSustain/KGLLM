# KG_LLM_Mapping
This repository contains files for task 1 and task 2. The structure of the project is as follows:

- **Task1**: Contains the primary files for Task 1.
  - **Data**: Contains the evaluation indicators and ESRS topic-specific standards.
  - **Result.xlsx**: The final results of the Task 1 in Excel format.
  - **Task1.ipynb**: The Jupyter Notebook used for Task 1 analysis and evaluation.

- **Task2**: Contains the primary files for Task 2.
  - **Data**: Contains the test GRI documents.
  - **Prompt**: Contains pre-difined instructions.
  - **Result.xlsx**: The final results of the Task 1 in Excel format.
  - **Task2.ipynb**: The Jupyter Notebook used for Task 1 analysis and evaluation.

## Task 1: Retrieval Performance Assessment

### Objective

Task 1 aims to assess the performance of the retrieval step in our data processing pipeline. Specifically, the task involves evaluating how effectively our system can retrieve relevant text chunks from ESRS documents with the sub-graphs Indicators from GRI standard documents.

### Methodology

1. **Framework and Models**:
   - We constructed the Retrieval-Augmented Generation (RAG) pipeline using the Langchain framework.

2. **Input Data**:
   - The input for each experimental Indicator A was a sub-graph extracted via a SPARQL query from the Knowledge Graph (KG) used in our previous work.

3. **Desired Output**:
   - The goal was to identify the chunk numbers from ESRS Standard that contain potential mapping candidates relevant to the given Indicator from GRI standard.

4. **Evaluation**:
   - The outputs generated by the language model (LLM) were compared against those determined by human experts.
   - Since the expected number of relevant results can vary for different indicators, we conducted multiple rounds of experimentation.
   - For each round, we set the number of chunks to be retrieved to `n`, `2n`, and `3n`, where `n` represents the number of expected results.

## Task 2: Entity and Relation Extraction Evaluation

### Objective

Task 2 was designed to evaluate the capabilities of LLMs in performing entity and relation extraction tasks. The primary goal was to extract indicator entities and four specific types of properties for each entity from the given input text. The properties of interest are:
- `hasQuantityKind`
- `hasUnit`
- `hasMeasurementPhenomenon`
- `hasApplicability`

### Methodology

1. **Prompt Design**:
   - The prompt was carefully crafted and consisted of four parts:
     - **Instructions**: Guidelines for the extraction task.
     - **Class and Property Explanations**: Definitions and descriptions derived from our previous work.
     - **Few-shot Examples**: Sample extractions reformulated from our Knowledge Graph (KG) to guide the model.
     - **Input Text**: The text from which the entities and properties were to be extracted.

2. **Model**:
   - We used the GPT-4 model to perform the extraction tasks.

3. **Evaluation**:
   - The results were compared against entities and triples annotated by human experts.
   - The experiment used input text derived from standards GRI 302, 305, and 306.
   - Human annotators labeled a total of 52 indicators and 214 triples from these standards to serve as the ground truth.

4. **Metrics**:
   - Precision, recall, and F1-scores were computed separately for entity and relation extraction. The detailed results are presented following Table.

        | Tasks                  | Precision | Recall | F1-Score |
        |-----------------------|-----------|--------|----------|
        | Indicator Extraction  | 0.72    | 0.67  | 0.70    |
        | Relation Extraction   | 0.67    | 0.58 | 0.62    |

      





