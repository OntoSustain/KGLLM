{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgraph enhance Text Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\" \n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.openai.com/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load ESRS pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"data/ESRS E1 Climate Change November 2022.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anacanda3\\envs\\graph_ontotext_graphdb_qa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "persist_directory = './chromadb'\n",
    "vectordb = Chroma.from_documents(documents=pages, embedding=embedding_function, persist_directory=persist_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract triples from graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., type, AbsoluteIndicator)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., measuresGRITopic, GRITopic3_305)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., hasUnit, TON_Metric)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., type, Indicator)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., identifier, \"305-1-a\")\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., type, NamedIndividual)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., hasApplicability, mandatory)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., prefLabel, \"Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent.\")\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., inDisclosure, Disclosure9_305-1)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., hasQuantityKind, Mass)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., isDefinedBy, GRI)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., measuresCoreTopic, CoreTopic3_Emi)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., label, \"Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent.\")\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., hasMeasurementVariable, GrossDirectScope1GHGEmissions)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., measuresDimension, Environment)\n",
      "(Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent., inStandard, Standard3_GRI305)\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef, Literal\n",
    "from rdflib.namespace import NamespaceManager, RDF, RDFS, SKOS, OWL\n",
    "\n",
    "# Define a function to extract and format triples from a Turtle file\n",
    "def extract_and_format_triples(file_path):\n",
    "    # Create an RDF graph\n",
    "    graph = Graph()\n",
    "    \n",
    "    # Read the Turtle file\n",
    "    with open(file_path, 'r') as file:\n",
    "        turtle_data = file.read()\n",
    "    \n",
    "    # Parse the Turtle data\n",
    "    graph.parse(data=turtle_data, format='ttl')\n",
    "    \n",
    "    # Add common namespaces\n",
    "    ns_manager = NamespaceManager(graph)\n",
    "    ns_manager.bind('rdf', RDF)\n",
    "    ns_manager.bind('rdfs', RDFS)\n",
    "    ns_manager.bind('skos', SKOS)\n",
    "    ns_manager.bind('owl', OWL)\n",
    "    \n",
    "    formatted_triples = []\n",
    "    \n",
    "    # Iterate through all triples in the graph\n",
    "    for subj, pred, obj in graph:\n",
    "        # Format each part of the triple\n",
    "        subj_str = format_entity(subj, ns_manager)\n",
    "        pred_str = format_entity(pred, ns_manager)\n",
    "        obj_str = format_entity(obj, ns_manager, is_object=True)\n",
    "        if pred_str == \"prefLabel\":\n",
    "            indicator_str = obj_str\n",
    "        \n",
    "        # Append the formatted triple to the list\n",
    "        formatted_triples.append((subj_str, pred_str, obj_str))\n",
    "\n",
    "    # Replace subjects in formatted_triples with indicator_str\n",
    "    if indicator_str:\n",
    "        indicator_str_clean = indicator_str.strip('\"')  # Remove quotes if indicator_str is a literal\n",
    "        formatted_triples = [(indicator_str_clean, pred, obj) for _, pred, obj in formatted_triples]\n",
    "    \n",
    "    # Format the triples as strings for final output\n",
    "    formatted_triples = [f\"({subj}, {pred}, {obj})\" for subj, pred, obj in formatted_triples]\n",
    "    \n",
    "    return formatted_triples\n",
    "\n",
    "# Helper function to format entities and remove specified prefixes\n",
    "def format_entity(entity, ns_manager, is_object=False):\n",
    "    if isinstance(entity, URIRef):\n",
    "        entity_str = None\n",
    "        \n",
    "        # Get a namespace prefix if available\n",
    "        for prefix, uri in ns_manager.namespaces():\n",
    "            if str(entity).startswith(str(uri)):\n",
    "                entity_str = str(entity).replace(str(uri), \"\")\n",
    "                break\n",
    "\n",
    "        if not entity_str:\n",
    "            # Fallback to last part of the URI\n",
    "            entity_str = entity.split(\"/\")[-1] if \"/\" in entity else str(entity)\n",
    "\n",
    "        # Remove specified prefixes\n",
    "        entity_str = remove_prefixes(entity_str)\n",
    "\n",
    "    elif isinstance(entity, Literal):\n",
    "        # Format literal values with quotes\n",
    "        entity_str = f'\"{entity}\"'\n",
    "    else:\n",
    "        entity_str = str(entity)\n",
    "\n",
    "    return entity_str\n",
    "\n",
    "# Function to remove specified prefixes\n",
    "def remove_prefixes(entity_str):\n",
    "    prefixes_to_remove = [\"rso:\", \"rdf:\", \"rdfs:\", \"skos:\", \"owl:\"]\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if entity_str.startswith(prefix):\n",
    "            entity_str = entity_str[len(prefix):]\n",
    "            break\n",
    "    return entity_str\n",
    "\n",
    "# Specify the path to your RDF Turtle file\n",
    "rdf_file_path = 'Data/Indicator3.ttl'\n",
    "\n",
    "# Extract and format triples from the Turtle file\n",
    "formatted_triples = extract_and_format_triples(rdf_file_path)\n",
    "\n",
    "# Print the formatted triples\n",
    "for triple in formatted_triples:\n",
    "    print(triple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Understanding and Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt for triple to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "triple_to_text_prompt = PromptTemplate(\n",
    "    input_variables=[\"triples\"],\n",
    "    template=(\n",
    "        \"You are a text generation assistant that helps in transforming RDF triples into natural language sentences. \"\n",
    "        \"An RDF triple consists of a subject, a predicate, and an object. Your task is to construct clear and concise sentences from the given triples. \"\n",
    "        \"Follow the format: subject, predicate, object. Use appropriate connecting words and verbs to form meaningful sentences. \"\n",
    "        \"Here are some definitions and guidelines:\\n\"\n",
    "        \n",
    "        \"Definition: A sustainability reporting indicator is defined as 'a single measurement from which quantitative conclusions on the environmental phenomenon can be inferred'.\\n\"\n",
    "        \n",
    "        \"There are 4 kinds of predicates that can be used for extracting triples:\\n\"\n",
    "        \"1. Has quantity kind: Relates the indicator to the measurement quantity kind.\\n\"\n",
    "        \"2. Has measurement phenomenon: Relates the indicator to the environmental phenomenon measured.\\n\"\n",
    "        \"3. Has unit: Relates the indicator to the required unit derived from sustainability standards.\\n\"\n",
    "        \"4. Has applicability: Relates the indicator to its applicability status, which can be either optional (indicated by 'if applicable') or mandatory (indicated by 'shall').\\n\"\n",
    "        \n",
    "        \"Make sure to explain the relationships in a way that a layperson can understand. Here are some examples:\\n\"\n",
    "        \n",
    "        \"Example 1:\\n\"\n",
    "        \"Input Triple: (CoreTopic4_Wst, type, CoreTopic)\\n\"\n",
    "        \"Output Sentence: 'CoreTopic4_Wst is a type of CoreTopic.'\\n\\n\"\n",
    "        \n",
    "        \"Example 2:\\n\"\n",
    "        \"Input Triple: (Total fuel consumption within the organization from non-renewable sources, in joules or multiples, and including fuel types used, inStandard, Standard3_GRI305)\\n\"\n",
    "        \"Output Sentence: 'Total fuel consumption within the organization from non-renewable sources, in joules or multiples, and including fuel types used, is included in the Standard3_GRI305.'\\n\\n\"\n",
    "\n",
    "        \"Now, given the following {triples}, construct text sentences:\\n\\n\"\n",
    "        \"Output Sentence:\\n\"\n",
    "    \n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anacanda3\\envs\\graph_ontotext_graphdb_qa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0) # gpt-3.5-turbo-16k, gpt-4-turbo\n",
    "\n",
    "triple2text_chain = LLMChain(llm=llm, prompt=triple_to_text_prompt)\n",
    "\n",
    "triples2text_list = []\n",
    "# Process each triple and generate text\n",
    "for triple in formatted_triples:\n",
    "    input_data = {\"triples\": triple}\n",
    "    output = triple2text_chain.invoke(input_data)\n",
    "    triples2text_list.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Text:\n",
      " 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent is a type of AbsoluteIndicator.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent measures the GRITopic3_305.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent are measured in metric tons.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent is a type of Indicator.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent is identified by the code \"305-1-a\".' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent is a type of NamedIndividual.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent are mandatory to report.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent' is labeled as \"Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent.\" 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent are disclosed in Disclosure9_305-1.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent measures the quantity kind of Mass.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent is defined by GRI.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent measures CoreTopic3_Emi.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent' is labeled as \"Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent.\" 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent measures the GrossDirectScope1GHGEmissions.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent measures the dimension of the environment.' 'Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent are included in Standard3_GRI305.' \n"
     ]
    }
   ],
   "source": [
    "# Initialize the concatenated text\n",
    "total_text = \"\"\n",
    "\n",
    "# Iterate over the list and concatenate the text values\n",
    "for triples2text in triples2text_list:\n",
    "    total_text += triples2text['text'] + \" \"\n",
    "\n",
    "# Print the concatenated text\n",
    "print(\"Concatenated Text:\\n\", total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for summarizing sentences\n",
    "summarize_sentences_prompt = PromptTemplate(\n",
    "    input_variables=[\"sentences\"],\n",
    "    template=(\n",
    "        \"You are a text generation assistant that helps summarize information. Your task is to summarize the following sentences into a concise paragraph and DON'T miss any informantion:\\n\\n\"\n",
    "        \"{sentences}\\n\\n\"\n",
    "        \"Summarize the sentences in a concise paragraph and provide only the summarized text.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summarize = LLMChain(llm=llm, prompt=summarize_sentences_prompt)\n",
    "final_text = text_summarize.invoke({\"sentences\", total_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gross direct (Scope 1) GHG emissions, measured in metric tons of CO2 equivalent, are a mandatory reporting metric defined by GRI and identified by the code \"305-1-a.\" This metric is classified as an AbsoluteIndicator, Indicator, and NamedIndividual, and it measures various aspects including the GRITopic3_305, CoreTopic3_Emi, GrossDirectScope1GHGEmissions, and the environmental dimension. It is quantified in terms of mass and is included in Disclosure9_305-1 and Standard3_GRI305. The emissions are specifically labeled as \"Gross direct (Scope 1) GHG emissions in metric tons of CO2 equivalent.\"\n"
     ]
    }
   ],
   "source": [
    "# Output the summarized text\n",
    "print(final_text['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.invoke(final_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 34, 36, 32, 33, 17, 31, 11, 35, 18, 19, 37, 38, 13, 4]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Function to extract and display pages number\n",
    "def extract_page_numbers(documents):\n",
    "    page_numbers = []\n",
    "    for document in documents:\n",
    "        metadata = document.metadata\n",
    "        page_number = metadata.get('page')\n",
    "        if page_number is not None:\n",
    "            page_numbers.append(page_number+1)\n",
    "    return page_numbers\n",
    "\n",
    "\n",
    "extract_page_numbers(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
